{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxis in New York\n",
    "\n",
    "The present notebook serves as a study of the taxis dataset of the city of New York, available [here](http://www.debs2015.org/call-grand-challenge.html).\n",
    "This study is done using all variants of Spark, mainly the dataframes/SQL API but also the base API.\n",
    "\n",
    "\n",
    "The first step is to define the necessary functions that will be used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import socket\n",
    "import math\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql import types as T\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "# Lazily instantiated global instance of SparkSession\n",
    "def getSparkSessionInstance(sparkConf):\n",
    "    if (\"sparkSessionSingletonInstance\" not in globals()):\n",
    "        globals()[\"sparkSessionSingletonInstance\"] = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(conf=sparkConf) \\\n",
    "            .getOrCreate()\n",
    "    return globals()[\"sparkSessionSingletonInstance\"]\n",
    "\n",
    "def create_row_df(line):\n",
    "    \"\"\"\n",
    "        Function that creates a Structured Row object representing a Row in a DataFrame\n",
    "        Params:\n",
    "            line - A line from the input file\n",
    "        Returns:\n",
    "            A Row object representing a row in a Dataframe\n",
    "    \"\"\"\n",
    "    #Field - Array_position\n",
    "\n",
    "    #pickup_dt - 0      fare_amount - 8\n",
    "    #dropoff_dt - 1     tip_amount - 9\n",
    "    #trip_time - 2      total_amount - 10\n",
    "    #trip_distance - 3  pickup_cell - 11\n",
    "    #pickup_long - 4    dropoff_cell - 12\n",
    "    #pickup_lat - 5     medallion = 13\n",
    "    #dropoff_long - 6\n",
    "    #dropoff_lat - 7\n",
    "    splitted_line = line.split(',')\n",
    "    return Row(\n",
    "        pickup_dt = splitted_line[2], dropoff_dt = splitted_line[3], trip_time = int(splitted_line[4]), \\\n",
    "        trip_distance = float(splitted_line[5]), pickup_long = float(splitted_line[6]), pickup_lat = float(splitted_line[7]), \\\n",
    "        dropoff_long = float(splitted_line[8]), dropoff_lat = float(splitted_line[9]), fare_amount = float(splitted_line[11]), \\\n",
    "        tip_amount = float(splitted_line[14]), total_amount = float(splitted_line[16]), pickup_cell = estimate_cellid(float(splitted_line[7]), float(splitted_line[6])), \\\n",
    "        dropoff_cell = estimate_cellid(float(splitted_line[9]), float(splitted_line[8])), medallion = splitted_line[0]\n",
    "        )\n",
    "\n",
    "\n",
    "def filter_lines(line):\n",
    "    \"\"\"\n",
    "        Function that filters out empty lines as well as lines that have coordinates as 0.0000 (non relevant points)\n",
    "        Params:\n",
    "            line - A line from the input file\n",
    "        Returns:\n",
    "            True if the line passed this condition, False otherwise\n",
    "    \"\"\"\n",
    "    splitted_line = line.split(',')\n",
    "\n",
    "    #Limits of the grid. Every point that is not between these coordinates will be considered as an outlier\n",
    "    lon_min = -74.916578\n",
    "    lon_max = -73.120784\n",
    "    lat_min = 40.129716\n",
    "    lat_max = 41.477183\n",
    "\n",
    "    return (\n",
    "        len(line) > 0) and \\\n",
    "        (float(splitted_line[6]) != 0) and \\\n",
    "        (float(splitted_line[8]) != 0 and \\\n",
    "        (float(splitted_line[6]) >= lon_min) and \\\n",
    "        (float(splitted_line[6]) <= lon_max) and \\\n",
    "        (float(splitted_line[7]) >= lat_min) and \\\n",
    "        (float(splitted_line[7]) <= lat_max) and \\\n",
    "        (float(splitted_line[8]) >= lon_min) and \\\n",
    "        (float(splitted_line[8]) <= lon_max) and \\\n",
    "        (float(splitted_line[9]) >= lat_min) and \\\n",
    "        (float(splitted_line[9]) <= lat_max)\n",
    "        )\n",
    "\n",
    "\n",
    "def create_row(line):\n",
    "    \"\"\"\n",
    "        Function that creates a structured tuple representing a row in a RDD\n",
    "        Params:\n",
    "            line - A line from the input file\n",
    "        Rerturns:\n",
    "            A Structured tuple with 14 positions\n",
    "    \"\"\"\n",
    "    #Field - Array_position\n",
    "\n",
    "    #pickup_dt - 0      fare_amount - 8\n",
    "    #dropoff_dt - 1     tip_amount - 9\n",
    "    #trip_time - 2      total_amount - 10\n",
    "    #trip_distance - 3  pickup_cell - 11\n",
    "    #pickup_long - 4    dropoff_cell - 12\n",
    "    #pickup_lat - 5     medallion = 13\n",
    "    #dropoff_long - 6\n",
    "    #dropoff_lat - 7\n",
    "    \n",
    "    splitted_line = line.split(',')\n",
    "    return (\n",
    "        splitted_line[2], splitted_line[3], int(splitted_line[4]), float(splitted_line[5]), float(splitted_line[6]), \\\n",
    "        float(splitted_line[7]), float(splitted_line[8]), float(splitted_line[9]), float(splitted_line[11]), \\\n",
    "        float(splitted_line[14]), float(splitted_line[16]), estimate_cellid(float(splitted_line[7]), float(splitted_line[6])),\\\n",
    "        estimate_cellid(float(splitted_line[9]), float(splitted_line[8])), splitted_line[0]\n",
    "    )\n",
    "\n",
    "def estimate_cellid(lat, lon):\n",
    "    \"\"\"\n",
    "        Function that estimates a cell ID given a latitude and longitude based on the coordinates of cell 1.1\n",
    "        Params:\n",
    "            lat - Input latitude for which to find the cellID\n",
    "            lon - Input longitude for which to fin the cellID\n",
    "        Returns:\n",
    "            A String such as 'xxx.xxx' representing the ID of the cell\n",
    "    \"\"\"\n",
    "    x0 = -74.913585 #longitude of cell 1.1\n",
    "    y0 = 41.474937  #latitude of cell 1.1\n",
    "    s = 500 #500 meters\n",
    "\n",
    "    delta_x = 0.005986 / 500.0  #Change in longitude coordinates per meter\n",
    "    delta_y = 0.004491556 /500.0    #Change in latitude coordinates per meter\n",
    "\n",
    "    cell_x = 1 + math.floor((1/2) + (lon - x0)/(s * delta_x))\n",
    "    cell_y = 1 + math.floor((1/2) + (y0 - lat)/(s * delta_y))\n",
    "    \n",
    "    return f\"{cell_x}.{cell_y}\"\n",
    "\n",
    "def process(time, rdd, query_name):\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    try:\n",
    "        # Get the singleton instance of SparkSession\n",
    "        spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "        \n",
    "        # Convert RDD[String] to RDD[Row] to DataFrame\n",
    "        rowRdd = rdd.map(lambda line: create_row_df(line))\n",
    "        #rowRdd = rdd.map(lambda line: Row(word=line))\n",
    "        df = spark.createDataFrame(rowRdd)\n",
    "\n",
    "        # This if block switches between the different queries\n",
    "        if query_name == \"Query1\":\n",
    "\n",
    "            # Obtain the unsorted list of route frequencies by\n",
    "            # grouping by a route (a pickup_cell and a dropoff_cell) and a window of 30 minutes\n",
    "            # then aggregate the values into a count (in this case counting fare amount for no particular reason)\n",
    "            route_freqs = df\\\n",
    "                        .groupBy(df.pickup_cell,\\\n",
    "                                 df.dropoff_cell,\\\n",
    "                                 window(df.pickup_dt, '30 minutes'))\\\n",
    "                        .agg(count(df.fare_amount).alias(\"NumTrips\"))\n",
    "\n",
    "            # Then select only the desired columns and sort descendingly the NumTrips variable and limit it to 10 values\n",
    "            # to get the 10 most frequent routes\n",
    "            most_freq_routes = route_freqs\\\n",
    "                        .select(route_freqs.pickup_cell, route_freqs.dropoff_cell, route_freqs.NumTrips)\\\n",
    "                        .sort(desc(\"NumTrips\"))\\\n",
    "                        .limit(10)\n",
    "\n",
    "            most_freq_routes.show()\n",
    "            \n",
    "        elif query_name == \"Query2\":\n",
    "\n",
    "            # Get the number of empty taxis in an area by obtaining the number of taxis that\n",
    "            # had a dropoff in a given cell in the last 30 minutes\n",
    "            empty_taxis = df\\\n",
    "                        .groupBy(df.dropoff_cell,\\\n",
    "                                 window(df.dropoff_dt, \"30 minutes\"))\\\n",
    "                        .count().alias(\"NumEmptyTaxis\")\n",
    "\n",
    "            # Get the total profit of an area which is the average\n",
    "            # of fare amount plus tip for a given area in the last 15 minutes\n",
    "            profit = df\\\n",
    "                    .groupBy(df.pickup_cell,\\\n",
    "                             window(df.pickup_dt, \"15 minutes\"))\\\n",
    "                    .agg(avg(df.fare_amount + df.tip_amount).alias(\"AreaProfit\"))\n",
    "            \n",
    "            # Join the two streams together on the commun column (the cell of pickup or dropoff)\n",
    "            joined_dfs = empty_taxis.join(profit, empty_taxis.dropoff_cell == profit.pickup_cell)\n",
    "            \n",
    "            # Creating a temporary view of this stream naming it 'joined_dfs'\n",
    "            joined_dfs.createOrReplaceTempView('joined_dfs')\n",
    "\n",
    "            # This SQL expression selects the a given cell and for that cell\n",
    "            # divides the total area profit by the number of empty taxis to obtain the profitability\n",
    "            sql_query = \"\"\"\n",
    "            SELECT pickup_cell, AreaProfit/count AS Profitability\n",
    "            FROM joined_dfs\n",
    "            WHERE count > 0\n",
    "            ORDER BY Profitability DESC\n",
    "            \"\"\"\n",
    "            profitability = spark.sql(sql_query)\n",
    "            \n",
    "            profitability.show()\n",
    "            \n",
    "        elif query_name == \"Query3\":\n",
    "            # Unfinished, completed with regular spark RDDs\n",
    "            dropoffs_aux = df\\\n",
    "                    .groupBy(df.medallion,\\\n",
    "                             df.dropoff_cell,\\\n",
    "                             window(df.dropoff_dt, \"1 hours\"))\\\n",
    "                    .agg(first(df.dropoff_dt).alias(\"ddt\"))\n",
    "\n",
    "            dropoffs = dropoffs_aux.select(dropoffs_aux.medallion,\\\n",
    "                                           dropoffs_aux.dropoff_cell,\\\n",
    "                                           dropoffs_aux.ddt)\n",
    "            \n",
    "            \n",
    "            pickups_aux = df\\\n",
    "                    .groupBy(df.medallion,\\\n",
    "                             df.pickup_cell,\\\n",
    "                             window(df.pickup_dt, \"1 hours\"))\\\n",
    "                    .agg(first(df.pickup_dt).alias(\"pdt\"))\n",
    "            \n",
    "            pickups = pickups_aux.select(pickups_aux.medallion,\\\n",
    "                                           pickups_aux.pickup_cell,\\\n",
    "                                           pickups_aux.pdt)\n",
    "            \n",
    "            conditions = [dropoffs.medallion == pickups.medallion]\n",
    "            joined_dfs = dropoffs.join(pickups, conditions)\n",
    "            \n",
    "            joined_dfs_correct = joined_dfs.where(joined_dfs.dropoff_cell == joined_dfs.pickup_cell)\n",
    "            joined_dfs_correct.show()\n",
    "            \n",
    "        elif query_name == \"Query4\":\n",
    "\n",
    "            def find_peak(durations, cells):\n",
    "                \"\"\"\n",
    "                    Aggregation function that receives a list of durations and cells\n",
    "                    Searches in the list of durations for a pattern where a given duration is higher\n",
    "                    than the one before and after it, and the duration after it is followed by 3 rides of increasing duration\n",
    "                \"\"\"\n",
    "                if(len(durations) == 0):\n",
    "                    return \"No Congested Areas, and no durations\"\n",
    "                \n",
    "                for idx, val in enumerate(durations):\n",
    "                    try:\n",
    "                        if idx != 0:\n",
    "                            if val > durations[idx - 1] and val > durations[idx + 1] and  durations[idx + 1] <  durations[idx + 2] and durations[idx + 2] <  durations[idx + 3]:\n",
    "                                return f\"Alert, area {cells[idx]} is congested\"\n",
    "                            else:\n",
    "                                return \"No Congested Areas\"\n",
    "                        else:\n",
    "                            return \"No congested Areas\"\n",
    "                    except:\n",
    "                        return \"No Congested Areas\"\n",
    "             \n",
    "            \n",
    "            # Registering the user defined function\n",
    "            find_peak_udf = functions.udf(find_peak, T.StringType())\n",
    "            \n",
    "            # Obtain all rides for a taxi in a day and aggregate the values with the custom aggregation function\n",
    "            # described above\n",
    "            grouped = df.groupBy(df.medallion, window(df.pickup_dt, \"1 days\")).agg(find_peak_udf(collect_list(df.trip_time), collect_list(df.pickup_cell)).alias(\"alert_message\"))\n",
    "            \n",
    "            grouped.select(grouped.alert_message).show()\n",
    "            \n",
    "        elif query_name == \"Query5\":\n",
    "\n",
    "            # Obtain an unsorted list of pleasant taxis by obtaining a list\n",
    "            # of taxis followed by the sum of the tips they obtained in a day\n",
    "            most_pleasant_unsorted = df\\\n",
    "                                .groupBy(df.medallion,\\\n",
    "                                         window(df.pickup_dt, \"1 days\"))\\\n",
    "                                .agg(functions.sum(df.tip_amount).alias(\"TotalTipAmount\"))\n",
    "            \n",
    "            # Sort this previous value descendingly and limit the stream to just 10 outputs\n",
    "            # to obtain the 10 most pleasant taxi drivers\n",
    "            most_pleasant = most_pleasant_unsorted\\\n",
    "                        .select(most_pleasant_unsorted.medallion, most_pleasant_unsorted.TotalTipAmount)\\\n",
    "                        .sort(desc(\"TotalTipAmount\"))\\\n",
    "                        .limit(10)\n",
    "            \n",
    "            most_pleasant.show()\n",
    "            \n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "\n",
    "def minutes_between(d1, d2):\n",
    "    d1 = datetime.strptime(d1, '%Y-%m-%d %H:%M:%S')\n",
    "    d2 = datetime.strptime(d2, '%Y-%m-%d %H:%M:%S')\n",
    "    return int((d2 - d1).seconds // 60 % 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Query - Get the 10 most frequent routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Creating the spark context and streaming context objects\n",
    "    sc = SparkContext(\"local[2]\", \"KafkaExample\")\n",
    "    ssc = StreamingContext(sc, 5)\n",
    "    \n",
    "    # Establsihing the Kafka conectoin subscribing to topic \"debs\" and listening on port 9092\n",
    "    lines = KafkaUtils.createDirectStream(ssc, [\"debs\"], \\\n",
    "                {\"metadata.broker.list\": \"kafka:9092\"})\n",
    "\n",
    "    # Kafka sends a timestamp followed by the actual value we're interested in\n",
    "    # so we just keep the second value in the tuple\n",
    "    lines = lines.map(lambda tup: tup[1])\n",
    "\n",
    "    # Filtering the lines according to our custom filter function\n",
    "    filtered_lines = lines.filter(lambda line: filter_lines(line))\n",
    "\n",
    "    ############### Query 1 ###############\n",
    "    # For each mini-batch RDD aplly the process function specifying the Query1\n",
    "    filtered_lines.foreachRDD(lambda time, rdd: process(time, rdd, \"Query1\"))\n",
    "\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "    ssc.stop()\n",
    "    sc.stop()\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "    ssc.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Query - Identify most profitable areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Creating the spark context and streaming context objects\n",
    "    sc = SparkContext(\"local[2]\", \"KafkaExample\")\n",
    "    ssc = StreamingContext(sc, 5)\n",
    "    \n",
    "    # Establsihing the Kafka conectoin subscribing to topic \"debs\" and listening on port 9092\n",
    "    lines = KafkaUtils.createDirectStream(ssc, [\"debs\"], \\\n",
    "                {\"metadata.broker.list\": \"kafka:9092\"})\n",
    "\n",
    "    # Kafka sends a timestamp followed by the actual value we're interested in\n",
    "    # so we just keep the second value in the tuple\n",
    "    lines = lines.map(lambda tup: tup[1])\n",
    "\n",
    "    # Filtering the lines according to our custom filter function\n",
    "    filtered_lines = lines.filter(lambda line: filter_lines(line))\n",
    "\n",
    "    ################ Query 2 ################\n",
    "    # For each mini-batch RDD aplly the process function specifying the Query2\n",
    "    filtered_lines.foreachRDD(lambda time, rdd: process(time, rdd, \"Query2\"))\n",
    "\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "    ssc.stop()\n",
    "    sc.stop()\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "    ssc.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Query - Alert when average idle time of a taxi is greater than a given amount of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Creating the spark context and streaming context objects\n",
    "    sc = SparkContext(\"local[2]\", \"KafkaExample\")\n",
    "    ssc = StreamingContext(sc, 5)\n",
    "    \n",
    "    # Establsihing the Kafka conectoin subscribing to topic \"debs\" and listening on port 9092\n",
    "    lines = KafkaUtils.createDirectStream(ssc, [\"debs\"], \\\n",
    "                {\"metadata.broker.list\": \"kafka:9092\"})\n",
    "\n",
    "    # Kafka sends a timestamp followed by the actual value we're interested in\n",
    "    # so we just keep the second value in the tuple\n",
    "    lines = lines.map(lambda tup: tup[1])\n",
    "\n",
    "    # Filtering the lines according to our custom filter function\n",
    "    filtered_lines = lines.filter(lambda line: filter_lines(line))\n",
    "    \n",
    "    ############### Query 3 ################\n",
    "    #filtered_lines.foreachRDD(lambda time, rdd: process(time, rdd, \"Query3\"))\n",
    "    \n",
    "    structured_lines = filtered_lines.map(lambda line: create_row(line))\n",
    "\n",
    "    # Creating a Window of 1 hour (60 seconds * 60)\n",
    "    # Then creating a (medallion, (pickup_dt, dropoff_dt, 1)) key-value pair\n",
    "    avg_idle_times = structured_lines \\\n",
    "                        .window(60 * 60) \\\n",
    "                        .map(lambda tup: (tup[13], (tup[0], tup[1], 1)))\n",
    "\n",
    "    # Calculating the minutes between the pickup_dt and dropoff_dt\n",
    "    # Then filtering the negative values (occurs when pick_dt is before dropoff_dt)\n",
    "    avg_idle_times = avg_idle_times \\\n",
    "                        .mapValues(lambda tup: (minutes_between(tup[1], tup[0]), 1)) \\\n",
    "                        .filter(lambda tup: int(tup[1][0] > 0))\n",
    "\n",
    "    # Summing the idle times to then make the average for each medallion\n",
    "    # Then calculating the average idle time for each medallion\n",
    "    avg_idle_times = avg_idle_times \\\n",
    "                        .reduceByKey(lambda acc, elem: (acc[0] + elem[0], acc[1] + elem[1])) \\\n",
    "                        .mapValues(lambda tup: tup[0] / tup[1])\n",
    "\n",
    "    # Retaining only the values that have an average idle team of over 10 minutes\n",
    "    # Then emmiting an alert message with the idle time of that medallion\n",
    "    avg_idle_times = avg_idle_times \\\n",
    "                        .filter(lambda tup: tup[1] > 10) \\\n",
    "                        .mapValues(lambda idle_time: f\"Idle time alert: {idle_time} minutes idle\")\n",
    "\n",
    "    avg_idle_times.pprint()\n",
    "\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "    ssc.stop()\n",
    "    sc.stop()\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "    ssc.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth query - Detect congested areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Creating the spark context and streaming context objects\n",
    "    sc = SparkContext(\"local[2]\", \"KafkaExample\")\n",
    "    ssc = StreamingContext(sc, 5)\n",
    "    \n",
    "    # Establsihing the Kafka conectoin subscribing to topic \"debs\" and listening on port 9092\n",
    "    lines = KafkaUtils.createDirectStream(ssc, [\"debs\"], \\\n",
    "                {\"metadata.broker.list\": \"kafka:9092\"})\n",
    "\n",
    "    # Kafka sends a timestamp followed by the actual value we're interested in\n",
    "    # so we just keep the second value in the tuple\n",
    "    lines = lines.map(lambda tup: tup[1])\n",
    "\n",
    "    # Filtering the lines according to our custom filter function\n",
    "    filtered_lines = lines.filter(lambda line: filter_lines(line))\n",
    "\n",
    "    ################ Query 4 ################\n",
    "    # For each mini-batch RDD aplly the process function specifying the Query4\n",
    "    filtered_lines.foreachRDD(lambda time, rdd: process(time, rdd, \"Query4\"))\n",
    "\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "    ssc.stop()\n",
    "    sc.stop()\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "    ssc.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth query - Select most pleasant taxi drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Creating the spark context and streaming context objects\n",
    "    sc = SparkContext(\"local[2]\", \"KafkaExample\")\n",
    "    ssc = StreamingContext(sc, 5)\n",
    "    \n",
    "    # Establsihing the Kafka conectoin subscribing to topic \"debs\" and listening on port 9092\n",
    "    lines = KafkaUtils.createDirectStream(ssc, [\"debs\"], \\\n",
    "                {\"metadata.broker.list\": \"kafka:9092\"})\n",
    "\n",
    "    # Kafka sends a timestamp followed by the actual value we're interested in\n",
    "    # so we just keep the second value in the tuple\n",
    "    lines = lines.map(lambda tup: tup[1])\n",
    "\n",
    "    # Filtering the lines according to our custom filter function\n",
    "    filtered_lines = lines.filter(lambda line: filter_lines(line))\n",
    "\n",
    "    ################ Query 5 ################\n",
    "    # For each mini-batch RDD aplly the process function specifying the Query5\n",
    "    filtered_lines.foreachRDD(lambda time, rdd: process(time, rdd, \"Query5\"))\n",
    "\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "    ssc.stop()\n",
    "    sc.stop()\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "    ssc.stop()\n",
    "    sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
